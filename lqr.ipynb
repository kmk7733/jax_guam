{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "ctx = jax.default_device(jax.devices(\"cpu\")[0])\n",
    "ctx.__enter__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jp\n",
    "from jax.tree_util import tree_map\n",
    "import numpy as np\n",
    "\n",
    "def lqr_continuous_time_infinite_horizon(A, B, Q, R, N = None):\n",
    "  # Take the last dimension, in case we try to do some kind of broadcasting\n",
    "  # thing in the future.\n",
    "  x_dim = A.shape[-1]\n",
    "#   Q.at[1,0].set(Q[0,1])\n",
    "#   Q[1,0]=Q[0,1]\n",
    "  Q  = Q.at[1,0].set(2*(Q[1,0]))\n",
    "  Q  = Q.at[0,1].set(2*(Q[0,1]))\n",
    "  Q = (Q.T+Q)/2\n",
    "\n",
    "  # See https://en.wikipedia.org/wiki/Linear%E2%80%93quadratic_regulator#Infinite-horizon,_continuous-time_LQR.\n",
    "  A1 = A - B @ jp.linalg.solve(R, N.T)\n",
    "  Q1 = Q - N @ jp.linalg.solve(R, N.T)\n",
    "\n",
    "  H = jp.block([[A1, -B @ jnp.linalg.inv(R)@B.T], [-Q1, -A1.T]])\n",
    "  eigvals, eigvectors = eig(H)\n",
    "  argsort = jp.argsort(eigvals)\n",
    "  ix = argsort[:x_dim]\n",
    "  U = eigvectors[:, ix]\n",
    "  P = U[x_dim:, :] @ jp.linalg.inv(U[:x_dim, :])\n",
    " \n",
    "  P = jp.real(P)\n",
    "  K = jp.linalg.inv(R)@B.T @ P\n",
    "  \n",
    "  return K\n",
    "  \n",
    "\n",
    "# def _test_lqr(n):\n",
    "#   import control\n",
    "#   from jax.tree_util import tree_map\n",
    "\n",
    "#   A = jp.eye(n)\n",
    "#   B = jp.eye(n)\n",
    "#   Q = jp.eye(n)\n",
    "#   R = jp.eye(n)\n",
    "#   N = jp.zeros((n, n))\n",
    "\n",
    "#   actual = lqr_continuous_time_infinite_horizon(A, B, Q, R, N)\n",
    "#   expected = control.lqr(A, B, Q, R, N)\n",
    "#   print(tree_map(jp.allclose, actual, expected))\n",
    "#   assert tree_map(jp.allclose, actual, expected)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#   _test_lqr(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax.lax.linalg as lax_linalg\n",
    "from jax import custom_jvp\n",
    "from functools import partial\n",
    "\n",
    "from jax import lax\n",
    "from jax.numpy.linalg import solve\n",
    "@custom_jvp\n",
    "def eig(a):\n",
    "    w, vl, vr = jax.numpy.linalg.eig(a)\n",
    "    return w, vr\n",
    "\n",
    "\n",
    "@eig.defjvp\n",
    "def eig_jvp_rule(primals, tangents):\n",
    "    a, = primals\n",
    "    da, = tangents\n",
    "\n",
    "    w, v = eig(a)\n",
    "\n",
    "    eye = jnp.eye(a.shape[-1], dtype=a.dtype)\n",
    "    # carefully build reciprocal delta-eigenvalue matrix, avoiding NaNs.\n",
    "    Fmat = (jnp.reciprocal(eye + w[..., jnp.newaxis, :] - w[..., jnp.newaxis])\n",
    "            - eye)\n",
    "    dot = partial(lax.dot if a.ndim == 2 else lax.batch_matmul,\n",
    "                  precision=lax.Precision.HIGHEST)\n",
    "    vinv_da_v = dot(solve(v, da), v)\n",
    "    du = dot(v, jnp.multiply(Fmat, vinv_da_v))\n",
    "    corrections = (jnp.conj(v) * du).sum(-2, keepdims=True)\n",
    "    dv = du - v * corrections\n",
    "    dw = jnp.diagonal(vinv_da_v, axis1=-2, axis2=-1)\n",
    "    return (w, v), (dw, dv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LQR Solution of python lqr library (P):\n",
      "[[ 9.99994540e-02  3.30118044e-04  5.16472351e-04  4.47218972e-01\n",
      "   8.69427500e-04  1.19560881e-03]\n",
      " [-3.30118323e-04  9.99994525e-02  5.48210595e-03  8.69427500e-04\n",
      "   2.87133415e-01 -2.26469153e-04]\n",
      " [-1.57599102e-06 -1.73412383e-05  3.16227761e+01  1.19560881e-03\n",
      "  -2.26469153e-04  7.88093457e+00]]\n",
      "---------------------dP/dR-----------------------\n",
      "------(1) Finite Difference Approximation---------\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'T'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 50\u001b[0m\n\u001b[1;32m     48\u001b[0m R \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39meye(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-4\u001b[39m\n\u001b[1;32m     49\u001b[0m N \u001b[38;5;241m=\u001b[39m jp\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 50\u001b[0m P_1 \u001b[38;5;241m=\u001b[39m \u001b[43mlqr_continuous_time_infinite_horizon\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(((P_1\u001b[38;5;241m-\u001b[39mP)\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m1e-4\u001b[39m))\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)))\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m----------(2) JAX Auto Differentiation------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[29], line 16\u001b[0m, in \u001b[0;36mlqr_continuous_time_infinite_horizon\u001b[0;34m(A, B, Q, R, N)\u001b[0m\n\u001b[1;32m     13\u001b[0m Q \u001b[38;5;241m=\u001b[39m (Q\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m+\u001b[39mQ)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# See https://en.wikipedia.org/wiki/Linear%E2%80%93quadratic_regulator#Infinite-horizon,_continuous-time_LQR.\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m A1 \u001b[38;5;241m=\u001b[39m A \u001b[38;5;241m-\u001b[39m B \u001b[38;5;241m@\u001b[39m jp\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39msolve(R, \u001b[43mN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m)\n\u001b[1;32m     17\u001b[0m Q1 \u001b[38;5;241m=\u001b[39m Q \u001b[38;5;241m-\u001b[39m N \u001b[38;5;241m@\u001b[39m jp\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39msolve(R, N\u001b[38;5;241m.\u001b[39mT)\n\u001b[1;32m     19\u001b[0m H \u001b[38;5;241m=\u001b[39m jp\u001b[38;5;241m.\u001b[39mblock([[A1, \u001b[38;5;241m-\u001b[39mB \u001b[38;5;241m@\u001b[39m jnp\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39minv(R)\u001b[38;5;129m@B\u001b[39m\u001b[38;5;241m.\u001b[39mT], [\u001b[38;5;241m-\u001b[39mQ1, \u001b[38;5;241m-\u001b[39mA1\u001b[38;5;241m.\u001b[39mT]])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'T'"
     ]
    }
   ],
   "source": [
    "# Define system matrices\n",
    "# A = jp.array([[0.0, 1.0], [-1.0, -1.0]])\n",
    "# B = jp.array([[0.0], [1.0]])\n",
    "# Q = jp.array([[1.0, 0.0], [0.0, 1.0]])\n",
    "# R = jp.array([[1.0]])\n",
    "# N = jp.zeros((2,1))\n",
    "# n=2\n",
    "# A = jp.eye(n)\n",
    "# B = jp.eye(n)\n",
    "# Q = jp.eye(n)\n",
    "# R = jp.eye(n)\n",
    "# N = jp.zeros((n, n))\n",
    "\n",
    "# A = jnp.array([[0, 1], [0, 0]])\n",
    "# B = jnp.array([[0], [1]])\n",
    "# Q = jnp.eye(2)\n",
    "# R = jnp.eye(1)\n",
    "# N = jp.zeros((2,1))\n",
    "\n",
    "Av = jnp.array([[0.0000,-0.0001,0.0002],[0.0030,-0.2047,-0.0202],[0.0012,-0.0002,-0.0721]])\n",
    "Bv = jnp.eye(3)\n",
    "Q = jnp.diag(np.array([0.01, 0.01, 1000.0, 0.0, 0.0, 0.0]))\n",
    "R = jnp.diag(np.array([1.0, 1.0, 1.0]))\n",
    "Cv = jnp.eye(3)\n",
    "Dv = jnp.zeros((3, 3))\n",
    "A = jnp.vstack(\n",
    "    (jnp.hstack((jnp.zeros((3, 3)), Cv)), jnp.hstack((jnp.zeros((3, 3)), Av)))\n",
    ")\n",
    "B = jnp.vstack((Dv, Bv))\n",
    "# P = lqr_continuous_time_infinite_horizon(A, B, Q, R)\n",
    "# print(\"LQR Solution (P):\")\n",
    "# print(P)\n",
    "# analytical_grad_P = jax.jacobian(lambda R: lqr_continuous_time_infinite_horizon(A, B, Q, R))(R)\n",
    "# analytical_grad_Q = jax.jacobian(lambda Q: lqr_continuous_time_infinite_horizon(A, B, Q, R))(Q)\n",
    "from control.matlab import *\n",
    "Ki, P, CLP = lqr(A, B, Q, R)\n",
    "\n",
    "print(\"LQR Solution of python lqr library (P):\")\n",
    "print(Ki)\n",
    "# print(analytical_grad_P.reshape((-1)))\n",
    "\n",
    "\n",
    "print(\"---------------------dP/dR-----------------------\")\n",
    "print(\"------(1) Finite Difference Approximation---------\")\n",
    "A = jnp.array([[0, 1], [0, 0]])\n",
    "B = jnp.array([[0], [1]])\n",
    "Q = jnp.eye(2)\n",
    "R = jnp.eye(1) + 1e-4\n",
    "N = jp.zeros((2,1))\n",
    "P_1 = lqr_continuous_time_infinite_horizon(A, B, Q, R)\n",
    "print(((P_1-P)/(1e-4)).reshape((-1)))\n",
    "print(\"----------(2) JAX Auto Differentiation------------\")\n",
    "print(analytical_grad_P.reshape((-1)))\n",
    "print(\"---------------------dP/dQ------------------------\")\n",
    "print(\"------(1) Finite Difference Approximation---------\")\n",
    "A = jnp.array([[0, 1], [0, 0]])\n",
    "B = jnp.array([[0], [1]])\n",
    "Q = jnp.array([[1+ 1e-4,0], [0, 1]])\n",
    "R = jnp.eye(1)\n",
    "N = jp.zeros((2,1))\n",
    "P_1 = lqr(A, B, Q, R)[1]\n",
    "print(((P_1-P)/(1e-4)).reshape((-1)))\n",
    "\n",
    "A = jnp.array([[0, 1], [0, 0]])\n",
    "B = jnp.array([[0], [1]])\n",
    "Q = jnp.array([[1,1e-4], [1e-4, 1]])\n",
    "R = jnp.eye(1)\n",
    "N = jp.zeros((2,1))\n",
    "P_1 = lqr(A, B, Q, R)[1]\n",
    "print(((P_1-P)/(1e-4)).reshape((-1)))\n",
    "\n",
    "\n",
    "A = jnp.array([[0, 1], [0, 0]])\n",
    "B = jnp.array([[0], [1]])\n",
    "Q = jnp.array([[1,1e-4], [1e-4, 1]])\n",
    "R = jnp.eye(1)\n",
    "N = jp.zeros((2,1))\n",
    "P_1 = lqr(A, B, Q, R)[1]\n",
    "print(((P_1-P)/(1e-4)).reshape((-1)))\n",
    "\n",
    "A = jnp.array([[0, 1], [0, 0]])\n",
    "B = jnp.array([[0], [1]])\n",
    "Q = jnp.array([[1,0], [0, 1+1e-4]])\n",
    "R = jnp.eye(1)\n",
    "N = jp.zeros((2,1))\n",
    "P_1 = lqr(A, B, Q, R)[1]\n",
    "print(((P_1-P)/(1e-4)).reshape((-1)))\n",
    "\n",
    "print(\"----------(2) JAX Auto Differentiation------------\")\n",
    "# print(analytical_grad_Q.reshape((4,4)).T)\n",
    "print(analytical_grad_Q[:,:, 0, 0].reshape((-1)))\n",
    "print(analytical_grad_Q[:,:, 0, 1].reshape((-1)))\n",
    "print(analytical_grad_Q[:,:, 1, 0].reshape((-1)))\n",
    "print(analytical_grad_Q[:,:, 1, 1].reshape((-1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Defines several utility functions.\n",
    "\n",
    "Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "\"\"\"\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as onp\n",
    "from jax.experimental import host_callback\n",
    "\n",
    "EPS_EIG = 1e-6\n",
    "\n",
    "\n",
    "def diag(x: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"A batch-compatible version of `numpy.diag`.\"\"\"\n",
    "    shape = x.shape + (x.shape[-1],)\n",
    "    y = jnp.zeros(shape, x.dtype)\n",
    "    i = jnp.arange(x.shape[-1])\n",
    "    return y.at[..., i, i].set(x)\n",
    "\n",
    "\n",
    "def angular_frequency_for_wavelength(wavelength: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"Returns the angular frequency for the specified wavelength.\"\"\"\n",
    "    return 2 * jnp.pi / wavelength  # Since by our convention c == 1.\n",
    "\n",
    "\n",
    "def matrix_adjoint(x: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"Computes the adjoint for a batch of matrices.\"\"\"\n",
    "    axes = tuple(range(x.ndim - 2)) + (x.ndim - 1, x.ndim - 2)\n",
    "    return jnp.conj(jnp.transpose(x, axes=axes))\n",
    "\n",
    "\n",
    "def batch_compatible_shapes(*shapes: Tuple[int, ...]) -> bool:\n",
    "    \"\"\"Returns `True` if all the shapes are batch-compatible.\"\"\"\n",
    "    max_dims = max([len(s) for s in shapes])\n",
    "    shapes = tuple([(1,) * (max_dims - len(s)) + s for s in shapes])\n",
    "    max_shape = [max(dim_shapes) for dim_shapes in zip(*shapes)]\n",
    "    for shape in shapes:\n",
    "        if any([a not in (1, b) for a, b in zip(shape, max_shape)]):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def atleast_nd(x: jnp.ndarray, n: int) -> jnp.ndarray:\n",
    "    \"\"\"Adds leading dimensions to `x`, ensuring that it is at least n-dimensional.\"\"\"\n",
    "    dims_to_add = tuple(range(max(0, n - x.ndim)))\n",
    "    return jnp.expand_dims(x, axis=dims_to_add)\n",
    "\n",
    "\n",
    "def absolute_axes(axes: Tuple[int, ...], ndim: int) -> Tuple[int, ...]:\n",
    "    \"\"\"Returns the absolute axes for given relative axes and number of array dimensions.\"\"\"\n",
    "    if not all(a in list(range(-ndim, ndim)) for a in axes):\n",
    "        raise ValueError(\n",
    "            f\"All elements of `axes` must be in the range ({ndim}, {ndim - 1}) \"\n",
    "            f\"but got {axes}.\"\n",
    "        )\n",
    "    absolute_axes = tuple([d % ndim for d in axes])\n",
    "    if len(absolute_axes) != len(set(absolute_axes)):\n",
    "        raise ValueError(\n",
    "            f\"Found duplicates in `axes`; computed absolute axes are {absolute_axes}.\"\n",
    "        )\n",
    "    return absolute_axes\n",
    "\n",
    "\n",
    "def interpolate_permittivity(\n",
    "    permittivity_solid: jnp.ndarray,\n",
    "    permittivity_void: jnp.ndarray,\n",
    "    density: jnp.ndarray,\n",
    ") -> jnp.ndarray:\n",
    "    \"\"\"Interpolates the permittivity with a scheme that avoids zero crossings.\n",
    "\n",
    "    The interpolation uses the scheme introduced in [2019 Christiansen], which avoids\n",
    "    zero crossings that can occur with metals or lossy materials having a negative\n",
    "    real component of the permittivity. https://doi.org/10.1016/j.cma.2018.08.034\n",
    "\n",
    "    Args:\n",
    "        permittivity_solid: The permittivity of solid regions.\n",
    "        permittivity_void: The permittivity of void regions.\n",
    "        density: The density, specifying which locations are solid and which are void.\n",
    "\n",
    "    Returns:\n",
    "        The interpolated permittivity.\n",
    "    \"\"\"\n",
    "    n_solid = jnp.real(jnp.sqrt(permittivity_solid))\n",
    "    k_solid = jnp.imag(jnp.sqrt(permittivity_solid))\n",
    "    n_void = jnp.real(jnp.sqrt(permittivity_void))\n",
    "    k_void = jnp.imag(jnp.sqrt(permittivity_void))\n",
    "    n = density * n_solid + (1 - density) * n_void\n",
    "    k = density * k_solid + (1 - density) * k_void\n",
    "    return (n + 1j * k) ** 2\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Functions related to a generalized eigensolve with custom vjp rule.\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "@jax.custom_vjp\n",
    "def eig(matrix: jnp.ndarray, eps: float = EPS_EIG) -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
    "    \"\"\"Wraps `jnp.linalg.eig` in a jit-compatible, differentiable manner.\n",
    "\n",
    "    The custom vjp allows gradients with resepct to the eigenvectors, unlike the\n",
    "    standard jax implementation of `eig`. We use an expression for the gradient\n",
    "    given in [2019 Boeddeker] along with a regularization scheme used in [2021\n",
    "    Colburn]. The method effectively applies a Lorentzian broadening to a term\n",
    "    containing the inverse difference of eigenvalues.\n",
    "\n",
    "    [2019 Boeddeker] https://arxiv.org/abs/1701.00392\n",
    "    [2021 Coluburn] https://www.nature.com/articles/s42005-021-00568-6\n",
    "\n",
    "    Args:\n",
    "        matrix: The matrix for which eigenvalues and eigenvectors are sought.\n",
    "        eps: Parameter which determines the degree of broadening.\n",
    "\n",
    "    Returns:\n",
    "        The eigenvalues and eigenvectors.\n",
    "    \"\"\"\n",
    "    del eps\n",
    "    return _eig_host(matrix)\n",
    "\n",
    "\n",
    "def _eig_host(matrix: jnp.ndarray) -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
    "    \"\"\"Wraps jnp.linalg.eig so that it can be jit-ed on a machine with GPUs.\"\"\"\n",
    "    eigenvalues_shape = jax.ShapeDtypeStruct(matrix.shape[:-1], complex)\n",
    "    eigenvectors_shape = jax.ShapeDtypeStruct(matrix.shape, complex)\n",
    "\n",
    "    def _eig_cpu(matrix: jnp.ndarray) -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
    "        # We force this computation to be performed on the cpu by jit-ing and\n",
    "        # explicitly specifying the device.\n",
    "        with jax.default_device(jax.devices(\"cpu\")[0]):\n",
    "            return jax.jit(jnp.linalg.eig)(matrix)\n",
    "\n",
    "    return host_callback.call(\n",
    "        _eig_cpu,\n",
    "        matrix.astype(complex),\n",
    "        result_shape=(eigenvalues_shape, eigenvectors_shape),\n",
    "    )\n",
    "\n",
    "\n",
    "def _eig_fwd(\n",
    "    matrix: jnp.ndarray,\n",
    "    eps: float,\n",
    ") -> Tuple[Tuple[jnp.ndarray, jnp.ndarray], Tuple[jnp.ndarray, jnp.ndarray, float]]:\n",
    "    \"\"\"Implements the forward calculation for `eig`.\"\"\"\n",
    "    eigenvalues, eigenvectors = _eig_host(matrix)\n",
    "    return (eigenvalues, eigenvectors), (eigenvalues, eigenvectors, eps)\n",
    "\n",
    "\n",
    "def _eig_bwd(\n",
    "    res: Tuple[jnp.ndarray, jnp.ndarray, float],\n",
    "    grads: Tuple[jnp.ndarray, jnp.ndarray],\n",
    ") -> Tuple[jnp.ndarray, None]:\n",
    "    \"\"\"Implements the backward calculation for `eig`.\"\"\"\n",
    "    eigenvalues, eigenvectors, eps = res\n",
    "    grad_eigenvalues, grad_eigenvectors = grads\n",
    "\n",
    "    # Compute the F-matrix, from equation 5 of [2021 Colburn]. This applies a\n",
    "    # Lorentzian broadening to the matrix `f = 1 / (eigenvalues[i] - eigenvalues[j])`.\n",
    "    eigenvalues_i = eigenvalues[..., jnp.newaxis, :]\n",
    "    eigenvalues_j = eigenvalues[..., :, jnp.newaxis]\n",
    "    f_broadened = (eigenvalues_i - eigenvalues_j) / (\n",
    "        (eigenvalues_i - eigenvalues_j) ** 2 + eps\n",
    "    )\n",
    "\n",
    "    # Manually set the diagonal elements to zero, as we do not use broadening here.\n",
    "    i = jnp.arange(f_broadened.shape[-1])\n",
    "    f_broadened = f_broadened.at[..., i, i].set(0)\n",
    "\n",
    "    # By jax convention, gradients are with respect to the complex parameters, not with\n",
    "    # respect to their conjugates. Take the conjugates.\n",
    "    grad_eigenvalues_conj = jnp.conj(grad_eigenvalues)\n",
    "    grad_eigenvectors_conj = jnp.conj(grad_eigenvectors)\n",
    "\n",
    "    eigenvectors_H = matrix_adjoint(eigenvectors)\n",
    "    dim = eigenvalues.shape[-1]\n",
    "    eye_mask = jnp.eye(dim, dtype=bool)\n",
    "    eye_mask = eye_mask.reshape((1,) * (eigenvalues.ndim - 1) + (dim, dim))\n",
    "\n",
    "    # Then, the gradient is found by equation 4.77 of [2019 Boeddeker].\n",
    "    rhs = (\n",
    "        diag(grad_eigenvalues_conj)\n",
    "        + jnp.conj(f_broadened) * (eigenvectors_H @ grad_eigenvectors_conj)\n",
    "        - jnp.conj(f_broadened)\n",
    "        * (eigenvectors_H @ eigenvectors)\n",
    "        @ jnp.where(eye_mask, jnp.real(eigenvectors_H @ grad_eigenvectors_conj), 0.0)\n",
    "    ) @ eigenvectors_H\n",
    "    grad_matrix = jnp.linalg.solve(eigenvectors_H, rhs)\n",
    "\n",
    "    # Take the conjugate of the gradient, reverting to the jax convention\n",
    "    # where gradients are with respect to complex parameters.\n",
    "    grad_matrix = jnp.conj(grad_matrix)\n",
    "\n",
    "    # Return `grad_matrix`, and `None` for the gradient with respect to `eps`.\n",
    "    return grad_matrix, None\n",
    "\n",
    "\n",
    "eig.defvjp(_eig_fwd, _eig_bwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.37841423, 0.41421356],\n",
       "       [0.41421356, 0.68179283]])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy\n",
    "import scipy.linalg\n",
    "lqr(A, B, Q, R)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LQR Solution (P): [[1.3784151  0.41421378]\n",
      " [0.4142142  0.6817929 ]]\n",
      "LQR Solution of python lqr library (P): [[1.37841423 0.41421356]\n",
      " [0.41421356 0.68179283]]\n",
      "[[[[ 8.9190531e-01 -5.0000006e-01]\n",
      "   [-5.0000006e-01  4.2044830e-01]]\n",
      "\n",
      "  [[ 3.5355335e-01 -1.4901161e-08]\n",
      "   [-1.4901161e-08  5.0121344e-08]]]\n",
      "\n",
      "\n",
      " [[[ 3.5355321e-01  1.4901161e-08]\n",
      "   [ 1.4901161e-08 -5.0121344e-08]]\n",
      "\n",
      "  [[ 2.1022405e-01 -1.5805060e-08]\n",
      "   [-1.5805060e-08  2.9730177e-01]]]]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "# from scipy.linalg import solve_continuous_are\n",
    "import numpy as np\n",
    "from jax.scipy.linalg import schur, inv\n",
    "def solve_continuous_are(a, b, q, r):\n",
    "    # Equivalent of np.dot(b, np.dot(g, b.conj().T))\n",
    "    g = inv(r)\n",
    "    g = jnp.dot(jnp.dot(b, g), b.conj().T)\n",
    "\n",
    "    # Construct the blocks of the Hamiltonian matrix\n",
    "    z11 = a\n",
    "    z12 = -1.0 * g\n",
    "    z21 = -1.0 * q\n",
    "    z22 = -1.0 * a.conj().T\n",
    "\n",
    "    # Combine the blocks to form the full Hamiltonian matrix\n",
    "    z = jnp.vstack((jnp.hstack((z11, z12)), jnp.hstack((z21, z22))))\n",
    "\n",
    "    # Compute the Schur decomposition\n",
    "    # Note: JAX does not support sorting eigenvalues, so this is an approximation.\n",
    "    # s, u = schur(z,)\n",
    "    s,u,_ =scipy.linalg.schur(z, sort='lhp')\n",
    "\n",
    "    # s, u = schur(z,)\n",
    "\n",
    "    # Extract the top-left and bottom-left blocks of U\n",
    "    (m, n) = u.shape\n",
    "    u11 = u[0:m//2, 0:n//2]\n",
    "    u21 = u[m//2:m, 0:n//2]\n",
    "\n",
    "    # Compute the inverse of U11\n",
    "    u11i = inv(u11)\n",
    "\n",
    "    # Return the product of U21 and U11 inverse\n",
    "    return jnp.dot(u21, u11i)\n",
    "\n",
    "@jax.custom_vjp\n",
    "def lqr_solution(A, B, Q, R):\n",
    "    \"\"\"\n",
    "    Returns the LQR solution P (Riccati matrix) and sets up implicit differentiation.\n",
    "    \n",
    "    Args:\n",
    "        A (jax.numpy.ndarray): State transition matrix.\n",
    "        B (jax.numpy.ndarray): Control input matrix.\n",
    "        Q (jax.numpy.ndarray): State cost matrix.\n",
    "        R (jax.numpy.ndarray): Control cost matrix.\n",
    "    \n",
    "    Returns:\n",
    "        P (jax.numpy.ndarray): Solution to the CARE, matrix P.\n",
    "    \"\"\"\n",
    "    P = solve_continuous_are(A, B, Q, R)\n",
    "\n",
    "    # Attach the custom VJP\n",
    "    return P\n",
    "\n",
    "def care_residual(P, A, B, Q, R):\n",
    "    \"\"\"CARE residual function, F(P; A, B, Q, R) = 0.\"\"\"\n",
    "    return A.T @ P + P @ A - P @ B @ jnp.linalg.inv(R) @ B.T @ P + Q\n",
    "\n",
    "# Define backward pass for custom VJP\n",
    "def lqr_solution_bwd(fwd_vars, out_grad):\n",
    "    P, A, B, Q, R = fwd_vars  # Unpack saved values\n",
    "    # Define the CARE residual function\n",
    "\n",
    "    # Compute Jacobians of the residual function with respect to each argument\n",
    "    dres_dp = jax.jacobian(care_residual, 0)(*fwd_vars)\n",
    "    dres_da = jax.jacobian(care_residual, 1)(*fwd_vars)\n",
    "    dres_db = jax.jacobian(care_residual, 2)(*fwd_vars)\n",
    "    dres_dq = jax.jacobian(care_residual, 3)(*fwd_vars)\n",
    "    dres_dr = jax.jacobian(care_residual, 4)(*fwd_vars)\n",
    "    \n",
    "    # Solve for the adjoint (Lagrange multiplier)\n",
    "    adj = jnp.linalg.tensorsolve(dres_dp.T, out_grad.T)\n",
    "    N = adj.ndim\n",
    "\n",
    "    # Compute the gradients for A, B, Q, and R\n",
    "    a_grad = -jnp.tensordot(dres_da.T, adj, N).T\n",
    "    b_grad = -jnp.tensordot(dres_db.T, adj, N).T\n",
    "    q_grad = -jnp.tensordot(dres_dq.T, adj, N).T\n",
    "    q_grad = (q_grad + q_grad.T) / 2 \n",
    "    r_grad = -jnp.tensordot(dres_dr.T, adj, N).T\n",
    "    r_grad = (r_grad + r_grad.T) / 2 \n",
    "\n",
    "    return (a_grad, b_grad, q_grad, r_grad)\n",
    "\n",
    "def lqr_solution_fwd(A,B,Q,R):\n",
    "    P = lqr_solution(A,B,Q,R)\n",
    "    return P, (P, A, B, Q, R)\n",
    "\n",
    "lqr_solution.defvjp(lqr_solution_fwd, lqr_solution_bwd)\n",
    "\n",
    "# Define system matrices\n",
    "A = jnp.array([[0.0, 1.0], [-1.0, -1.0]])\n",
    "B = jnp.array([[0.0], [1.0]])\n",
    "Q = jnp.array([[1.0, 0.0], [0.0, 1.0]])\n",
    "R = jnp.array([[1.0]])\n",
    "\n",
    "# Get the LQR solution and implicit differentiation function\n",
    "P_solution = lqr_solution(A, B, Q, R)\n",
    "print(\"LQR Solution (P):\", P_solution)\n",
    "\n",
    "from control.matlab import *\n",
    "Kc, P, CLP = lqr(A, B, Q, R)\n",
    "\n",
    "print(\"LQR Solution of python lqr library (P):\", P)\n",
    "print(jax.jacobian(lambda Q: lqr_solution(A, B, Q, R))(Q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LQR Solution (P): [[0.99999994 0.        ]\n",
      " [0.         0.41421354]]\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "def lqr_continuous_time_infinite_horizon(A, B, Q, R, N):\n",
    "  # Take the last dimension, in case we try to do some kind of broadcasting\n",
    "  # thing in the future.\n",
    "  x_dim = A.shape[-1]\n",
    "\n",
    "  # See https://en.wikipedia.org/wiki/Linear%E2%80%93quadratic_regulator#Infinite-horizon,_continuous-time_LQR.\n",
    "  A1 = A - B @ jp.linalg.solve(R, N.T)\n",
    "  Q1 = Q - N @ jp.linalg.solve(R, N.T)\n",
    "\n",
    "  # See https://en.wikipedia.org/wiki/Algebraic_Riccati_equation#Solution.\n",
    "  H = jp.block([[A1, -B @ jp.linalg.solve(R, B.T)], [-Q1, -A1]])\n",
    "  # print(tree_map(jp.allclose, H.T, H))\n",
    "  # print(H)\n",
    "  if tree_map(jp.allclose, H.T, H):\n",
    "    eigvals, eigvectors = jp.linalg.eigh(H)\n",
    "  else:\n",
    "    eigvals, eigvectors = jp.linalg.eigh(H)\n",
    "  argsort = jp.argsort(eigvals)\n",
    "\n",
    "  ix = argsort[:x_dim]\n",
    "  U = eigvectors[:, ix]\n",
    "  P = U[x_dim:, :] @ jp.linalg.inv(U[:x_dim, :])\n",
    "\n",
    "  K = jp.linalg.solve(R, (B.T @ P + N.T))\n",
    "  return K, P, eigvals[ix]\n",
    "\n",
    "# Define system matrices\n",
    "A = jnp.array([[0.0, 1.0], [-1.0, -1.0]])\n",
    "B = jnp.array([[0.0], [1.0]])\n",
    "Q = jnp.array([[1.0, 0.0], [0.0, 1.0]])\n",
    "R = jnp.array([[1.0]])\n",
    "N = jnp.zeros((2,1))\n",
    "# Get the LQR solution and implicit differentiation function\n",
    "Kc, P, CLP = lqr_continuous_time_infinite_horizon(A, B, Q, R,N)\n",
    "print(\"LQR Solution (P):\", P)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LQR Solution of python lqr library (P): [[0.41421356 0.68179283]] [[1.37841423 0.41421356]\n",
      " [0.41421356 0.68179283]] [-0.8408964+0.8408964j -0.8408964-0.8408964j]\n",
      "[-0.8408964+0.8408964j -0.8408964-0.8408964j]\n"
     ]
    }
   ],
   "source": [
    "from control.matlab import *\n",
    "Kc, P, CLP = lqr(A, B, Q, R)\n",
    "\n",
    "print(\"LQR Solution of python lqr library (P):\", Kc, P, CLP)\n",
    "print(CLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=2\n",
    "n=2\n",
    "A = jnp.array([[0.0, 1.0], [-1.0, -1.0]])\n",
    "B = jnp.array([[0.0], [1.0]])\n",
    "Q = jnp.array([[1.0, 0.0], [0.0, 1.0]])\n",
    "R = jnp.array([[1.0]])\n",
    "N = jnp.zeros((2,1))\n",
    "H = np.zeros((4,4))\n",
    "H[:m, :m] = A\n",
    "H[:m, m:2 * m] = 0.\n",
    "H[:m, 2 * m:] = B\n",
    "H[m:2 * m, :m] = -Q\n",
    "H[m:2 * m, m:2 * m] = -A.conj().T\n",
    "H[m:2 * m, 2 * m:] = 0\n",
    "H[2 * m:, :m] = 0\n",
    "H[2 * m:, m:2 * m] = B.conj().T\n",
    "H[2 * m:, 2 * m:] = R\n",
    "\n",
    "J = jnp.array(block_diag(np.eye(2 * m), np.zeros_like(R)))\n",
    "q, r = jax.scipy.linalg.qr(H[:, -n:])\n",
    "H = q[:, n:].conj().T.dot(H[:, :2 * m])\n",
    "J = q[:2 * m, n:].conj().T.dot(J[:2 * m, :2 * m])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.84089642+0.84089642j -0.84089642-0.84089642j]\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "A = np.array([[0.0, 1.0], [-1.0, -1.0]])\n",
    "B = np.array([[0.0], [1.0]])\n",
    "Q = np.array([[1.0, 0.0], [0.0, 1.0]])\n",
    "R = np.array([[1.0]])\n",
    "N = np.zeros((2,1))\n",
    "X = scipy.linalg.solve_continuous_are(A, B, Q, R)\n",
    "K = np.linalg.solve(R, B.T @ X)\n",
    "E, _ = np.linalg.eig(A - B @ K)\n",
    "print(E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Numerical gradient of P with respect to Q:\n",
      "[[[[ 0.5671382  -0.43272972]\n",
      "   [-0.43272972  0.13142824]]\n",
      "\n",
      "  [[ 0.1899898   0.1899898 ]\n",
      "   [ 0.1899898  -0.05759299]]]\n",
      "\n",
      "\n",
      " [[[ 0.1899898   0.1899898 ]\n",
      "   [ 0.1899898  -0.05759299]]\n",
      "\n",
      "  [[ 0.1013279   0.1013279 ]\n",
      "   [ 0.1013279   0.23558736]]]]\n",
      "\n",
      "Numerical gradient of P with respect to R:\n",
      "[[[[0.00685453]]\n",
      "\n",
      "  [[0.00536442]]]\n",
      "\n",
      "\n",
      " [[[0.00536442]]\n",
      "\n",
      "  [[0.04097819]]]]\n"
     ]
    }
   ],
   "source": [
    "# Finite difference check for each entry of Q and R\n",
    "def numerical_jacobian(f, x, epsilon=1e-4):\n",
    "    \"\"\"Compute numerical Jacobian for each entry of x on matrix output f(x).\"\"\"\n",
    "    jacobian = jnp.zeros((f(x).shape[0], f(x).shape[1], x.shape[0], x.shape[1]))\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x_perturb_plus = x.at[i, j].set(x[i, j] + epsilon)\n",
    "            x_perturb_minus = x.at[i, j].set(x[i, j] - epsilon)\n",
    "            if i != j:\n",
    "                x_perturb_plus = x_perturb_plus.at[j,i].set(x_perturb_plus[j,i] + epsilon)\n",
    "                x_perturb_minus = x_perturb_minus.at[j,i].set(x_perturb_minus[j,i] - epsilon) \n",
    "            f_plus = f(x_perturb_plus)\n",
    "            f_minus = f(x_perturb_minus)\n",
    "            jacobian = jacobian.at[:, :, i, j].set((f_plus - f_minus) / (2 * epsilon))\n",
    "    return jacobian\n",
    "\n",
    "# Define function to get P with fixed A, B\n",
    "def get_P_with_Q(Q):\n",
    "    return lqr_solution(A, B, Q, R)\n",
    "\n",
    "def get_P_with_R(R):\n",
    "    return lqr_solution(A, B, Q, R)\n",
    "\n",
    "# Compute numerical gradients\n",
    "numerical_grad_Q = numerical_jacobian(get_P_with_Q, Q)\n",
    "numerical_grad_R = numerical_jacobian(get_P_with_R, R)\n",
    "\n",
    "print(\"\\nNumerical gradient of P with respect to Q:\")\n",
    "print(numerical_grad_Q)\n",
    "\n",
    "print(\"\\nNumerical gradient of P with respect to R:\")\n",
    "print(numerical_grad_R)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
