{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax.scipy.linalg import block_diag\n",
    "import jax\n",
    "\n",
    "def solve_continuous_are_jax(A, B, Q, R):\n",
    "    \"\"\"\n",
    "    Solves the continuous-time algebraic Riccati equation (CARE) using JAX.\n",
    "    \n",
    "    Parameters:\n",
    "        A: (M, M) array\n",
    "        B: (M, N) array\n",
    "        Q: (M, M) array\n",
    "        R: (N, N) array (nonsingular)\n",
    "        e: (M, M) array, optional\n",
    "        s: (M, N) array, optional\n",
    "        balanced: bool, optional (default=True)\n",
    "        \n",
    "    Returns:\n",
    "        X: (M, M) ndarray, solution to the CARE\n",
    "    \"\"\"\n",
    "    M, N = A.shape[0], B.shape[1]\n",
    "    \n",
    "    # Default matrices for e and s\n",
    "    e = jnp.eye(M)\n",
    "    s = jnp.zeros((M, N))\n",
    "    \n",
    "    # Construct Hamiltonian matrix H and identity matrix J\n",
    "    H = jnp.zeros((2 * M + N, 2 * M + N))\n",
    "    H = H.at[:M, :M].set(A)\n",
    "    H = H.at[:M, 2 * M:].set(B)\n",
    "    H = H.at[M:2 * M, :M].set(-Q)\n",
    "    H = H.at[M:2 * M, M:2 * M].set(jnp.conjugate(-A).T)\n",
    "    H = H.at[2 * M:, M:2 * M].set(B.conjugate().T)\n",
    "    H = H.at[2 * M:, 2 * M:].set(R)\n",
    "\n",
    "    J = block_diag(jnp.eye(2*M), jnp.zeros_like(R))\n",
    "    \n",
    "    # Balancing step\n",
    "    M_abs = jnp.abs(H) + jnp.abs(J)\n",
    "    M_abs = M_abs - jnp.diag(jnp.diag(M_abs))  # Remove diagonal elements to avoid scaling them\n",
    "\n",
    "    # Calculate scaling factors by summing row and column norms\n",
    "    row_scales = jnp.sum(M_abs, axis=1)\n",
    "    col_scales = jnp.sum(M_abs, axis=0)\n",
    "\n",
    "    # Avoid division by zero in scaling by adding a small value\n",
    "    scale_factors = jnp.sqrt(row_scales * col_scales)\n",
    "\n",
    "    # Check if scaling is required\n",
    "    if not jnp.allclose(scale_factors, jnp.ones_like(scale_factors)):\n",
    "        # Transform scale_factors into a logarithmic scale as per the original logic\n",
    "        log_scales = jnp.log2(scale_factors)\n",
    "        log_scales_left = log_scales[:M]\n",
    "        log_scales_right = log_scales[M:2*M]\n",
    "        s = jnp.round((log_scales_right - log_scales_left) / 2)\n",
    "        # Construct a scaling vector in the form [D, inv(D)] to apply element-wise\n",
    "        sca = 2 ** jnp.concatenate([s, -s, log_scales[2*M:]])\n",
    "        elwisescale = sca[:, None] * jnp.reciprocal(sca)\n",
    "\n",
    "        # Apply the scaling element-wise to H and J\n",
    "        H = H * elwisescale\n",
    "        J = J * elwisescale\n",
    "    \n",
    "    \n",
    "    # QR decomposition approximation for Hamiltonian matrix\n",
    "    q, r = jax.scipy.linalg.qr(H[:, -N:])\n",
    "    H = jnp.dot(q[:, N:].conjugate().T , H[:, :2 * M])\n",
    "    J = jnp.dot(q[:2*M, N:].conjugate().T , J[:2 * M, :2 * M])\n",
    "    \n",
    "    # Eigenvalue decomposition of the Hamiltonian matrix\n",
    "    J_inv = jnp.linalg.inv(J)\n",
    "    H_transformed = J_inv @ H\n",
    "    eigvals, eigvecs = jnp.linalg.eigh(H_transformed)\n",
    "    stable_indices = jnp.where(jnp.real(eigvals) < 0)[0]  # Stable subspace\n",
    "\n",
    "    # Select relevant eigenvectors for the stable subspace\n",
    "    U = eigvecs[:, stable_indices]\n",
    "    U00 = U[:M, :M]\n",
    "    U10 = U[M:, :M]\n",
    "    print(U00)\n",
    "    up, ul, uu =  jax.scipy.linalg.lu(U00)\n",
    "    x = jax.scipy.linalg.solve_triangular(ul.conjugate().T, \n",
    "                                          jax.scipy.linalg.solve_triangular(uu.conjugate().T, U10.conjugate().T, lower=True),\n",
    "                                          unit_diagonal=True\n",
    "                                        ).conjugate().T.dot(up.conjugate().T)\n",
    "    x = x * sca[:M, None] * sca[:M]\n",
    "\n",
    "    # Symmetrize the solution\n",
    "    return (x + x.conjugate().T) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "# from scipy.linalg import solve_continuous_are\n",
    "from jax import custom_jvp\n",
    "import numpy as np\n",
    "from scipy.linalg import block_diag, lu, solve_triangular, matrix_balance, qr, ordqz\n",
    "from numpy.linalg import inv, LinAlgError, norm, cond, svd\n",
    "\n",
    "def solve_continuous_are(a, b, q, r, e=None, s=None, balanced=True):\n",
    "    a, b, q, r, e, s, m, n, r_or_c, gen_are = _are_validate_args(a, b, q, r, e, s, 'care')\n",
    "\n",
    "    H = np.empty((2 * m + n, 2 * m + n), dtype=r_or_c)\n",
    "    H[:m, :m] = a\n",
    "    H[:m, m:2 * m] = 0.\n",
    "    H[:m, 2 * m:] = b\n",
    "    H[m:2 * m, :m] = -q\n",
    "    H[m:2 * m, m:2 * m] = -a.conj().T\n",
    "    H[m:2 * m, 2 * m:] = 0. if s is None else -s\n",
    "    H[2 * m:, :m] = 0. if s is None else s.conj().T\n",
    "    H[2 * m:, m:2 * m] = b.conj().T\n",
    "    H[2 * m:, 2 * m:] = r\n",
    "\n",
    "    if gen_are and e is not None:\n",
    "        J = block_diag(e, e.conj().T, np.zeros_like(r, dtype=r_or_c))\n",
    "    else:\n",
    "        J = block_diag(np.eye(2 * m), np.zeros_like(r, dtype=r_or_c))\n",
    "\n",
    "    if balanced:\n",
    "        M = np.abs(H) + np.abs(J)\n",
    "\n",
    "        np.fill_diagonal(M, 0.)\n",
    "        _, (sca, _) = matrix_balance(M, separate=1, permute=0)\n",
    "        if not np.allclose(sca, np.ones_like(sca)):\n",
    "            sca = np.log2(sca)\n",
    "            s = np.round((sca[m:2 * m] - sca[:m]) / 2)\n",
    "            sca = 2 ** np.r_[s, -s, sca[2 * m:]]\n",
    "            elwisescale = sca[:, None] * np.reciprocal(sca)\n",
    "            H *= elwisescale\n",
    "            J *= elwisescale\n",
    "    \n",
    "    q, r = qr(H[:, -n:])\n",
    "    H = q[:, n:].conj().T.dot(H[:, :2 * m])\n",
    "    J = q[:2 * m, n:].conj().T.dot(J[:2 * m, :2 * m])\n",
    "    \n",
    "    out_str = 'real' if r_or_c == float else 'complex'\n",
    "    _, _, _, _, _, u = ordqz(H, J, sort='lhp', overwrite_a=True, overwrite_b=True, check_finite=False, output=out_str)\n",
    "\n",
    "    if e is not None:\n",
    "        u, _ = qr(np.vstack((e.dot(u[:m, :m]), u[m:, :m])))\n",
    "    u00 = u[:m, :m]\n",
    "    u10 = u[m:, :m]\n",
    "    print(u00)\n",
    "    up, ul, uu = lu(u00)\n",
    "    if 1 / cond(uu) < np.spacing(1.):\n",
    "        raise np.linalg.LinAlgError('Failed to find a finite solution.')\n",
    "\n",
    "    x = solve_triangular(ul.conj().T, solve_triangular(uu.conj().T, u10.conj().T, lower=True), unit_diagonal=True).conj().T.dot(up.conj().T)\n",
    "    if balanced:\n",
    "        x *= sca[:m, None] * sca[:m]\n",
    "\n",
    "    u_sym = u00.conj().T.dot(u10)\n",
    "    n_u_sym = norm(u_sym, 1)\n",
    "    u_sym = u_sym - u_sym.conj().T\n",
    "    sym_threshold = np.max([np.spacing(1000.), 0.1 * n_u_sym])\n",
    "\n",
    "    if norm(u_sym, 1) > sym_threshold:\n",
    "        raise np.linalg.LinAlgError('The associated Hamiltonian pencil has eigenvalues too close to the imaginary axis')\n",
    "\n",
    "    return (x + x.conj().T) / 2\n",
    "\n",
    "def _are_validate_args(a, b, q, r, e, s, eq_type):\n",
    "    m, n = a.shape[0], b.shape[1]\n",
    "    r_or_c = np.common_type(a, b, q, r)\n",
    "    gen_are = e is not None or s is not None\n",
    "    if e is None:\n",
    "        e = np.eye(m, dtype=r_or_c)\n",
    "    if s is None:\n",
    "        s = np.zeros((m, n), dtype=r_or_c)\n",
    "    return a, b, q, r, e, s, m, n, r_or_c, gen_are\n",
    "\n",
    "\n",
    "def solve_care(A, B, Q, R):\n",
    "    \"\"\"\n",
    "    Wrapper to solve the continuous-time Algebraic Riccati Equation (CARE).\n",
    "    This uses SciPy's solver but converts results to JAX arrays.\n",
    "    \"\"\"\n",
    "    P = solve_continuous_are(A, B, Q, R)\n",
    "    return jnp.array(P)\n",
    "\n",
    "@jax.custom_vjp\n",
    "def lqr_solution(A, B, Q, R):\n",
    "    \"\"\"\n",
    "    Returns the LQR solution P (Riccati matrix) and sets up implicit differentiation.\n",
    "    \n",
    "    Args:\n",
    "        A (jax.numpy.ndarray): State transition matrix.\n",
    "        B (jax.numpy.ndarray): Control input matrix.\n",
    "        Q (jax.numpy.ndarray): State cost matrix.\n",
    "        R (jax.numpy.ndarray): Control cost matrix.\n",
    "    \n",
    "    Returns:\n",
    "        P (jax.numpy.ndarray): Solution to the CARE, matrix P.\n",
    "    \"\"\"\n",
    "    P = solve_care(A, B, Q, R)\n",
    "\n",
    "    # Attach the custom VJP\n",
    "    return P\n",
    "\n",
    "def care_residual(P, A, B, Q, R):\n",
    "    \"\"\"CARE residual function, F(P; A, B, Q, R) = 0.\"\"\"\n",
    "    return A.T @ P + P @ A - P @ B @ jnp.linalg.inv(R) @ B.T @ P + Q\n",
    "\n",
    "# Define backward pass for custom VJP\n",
    "def lqr_solution_bwd(fwd_vars, out_grad):\n",
    "    P, A, B, Q, R = fwd_vars  # Unpack saved values\n",
    "    # Define the CARE residual function\n",
    "\n",
    "    # Compute Jacobians of the residual function with respect to each argument\n",
    "    dres_dp = jax.jacobian(care_residual, 0)(*fwd_vars)\n",
    "    dres_da = jax.jacobian(care_residual, 1)(*fwd_vars)\n",
    "    dres_db = jax.jacobian(care_residual, 2)(*fwd_vars)\n",
    "    dres_dq = jax.jacobian(care_residual, 3)(*fwd_vars)\n",
    "    dres_dr = jax.jacobian(care_residual, 4)(*fwd_vars)\n",
    "    \n",
    "    # Solve for the adjoint (Lagrange multiplier)\n",
    "    adj = jnp.linalg.tensorsolve(dres_dp.T, out_grad.T)\n",
    "    N = adj.ndim\n",
    "\n",
    "    # Compute the gradients for A, B, Q, and R\n",
    "    a_grad = -jnp.tensordot(dres_da.T, adj, N).T\n",
    "    b_grad = -jnp.tensordot(dres_db.T, adj, N).T\n",
    "    q_grad = -jnp.tensordot(dres_dq.T, adj, N).T\n",
    "    q_grad = (q_grad + q_grad.T) / 2 \n",
    "    r_grad = -jnp.tensordot(dres_dr.T, adj, N).T\n",
    "    r_grad = (r_grad + r_grad.T) / 2 \n",
    "\n",
    "    return (a_grad, b_grad, q_grad, r_grad)\n",
    "\n",
    "def lqr_solution_fwd(A,B,Q,R):\n",
    "    P = lqr_solution(A,B,Q,R)\n",
    "    return P, (P, A, B, Q, R)\n",
    "\n",
    "lqr_solution.defvjp(lqr_solution_fwd, lqr_solution_bwd)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.47035969-0.17856214j -0.25435849+0.250803j  ]\n",
      " [ 0.24537151+0.54567603j -0.44688663-0.39617092j]]\n",
      "LQR Solution (P): [[1.3784142 +0.0000000e+00j 0.41421357+1.3877788e-17j]\n",
      " [0.41421357-1.3877788e-17j 0.6817928 +0.0000000e+00j]]\n",
      "[[0.         0.70710677]\n",
      " [0.9238795  0.        ]]\n",
      "LQR Solution of python jax (P): [[0.99999994 0.        ]\n",
      " [0.         0.41421354]]\n"
     ]
    }
   ],
   "source": [
    "# Define system matrices\n",
    "A = jnp.array([[0.0, 1.0], [-1.0, -1.0]])\n",
    "B = jnp.array([[0.0], [1.0]])\n",
    "Q = jnp.array([[1.0, 0.0], [0.0, 1.0]])\n",
    "R = jnp.array([[1.0]])\n",
    "\n",
    "# Get the LQR solution and implicit differentiation function\n",
    "P_solution = lqr_solution(A, B, Q, R)\n",
    "print(\"LQR Solution (P):\", P_solution)\n",
    "\n",
    "from control.matlab import *\n",
    "Kc, P, CLP = lqr(A, B, Q, R)\n",
    "\n",
    "# print(\"LQR Solution of python lqr library (P):\", P)\n",
    "print(\"LQR Solution of python jax (P):\", solve_continuous_are_jax(A,B,Q,R))\n",
    "\n",
    "# Analytical gradient of P with respect to Q and R (full Jacobian)\n",
    "# analytical_grad_Q = jax.jacobian(lambda Q: lqr_solution(A, B, Q, R))(Q)\n",
    "# analytical_grad_R = jax.jacobian(lambda R: lqr_solution(A, B, Q, R))(R)\n",
    "\n",
    "# print(\"\\nAnalytical Jacobian of P with respect to Q:\")\n",
    "# print(analytical_grad_Q)\n",
    "\n",
    "# print(\"\\nAnalytical Jacobian of P with respect to R:\")\n",
    "# print(analytical_grad_R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.47035969-0.17856214j -0.25435849+0.250803j  ]\n",
      " [ 0.24537151+0.54567603j -0.44688663-0.39617092j]]\n",
      "[[-0.47035969-0.17856214j -0.25435849+0.250803j  ]\n",
      " [ 0.24537151+0.54567603j -0.44688663-0.39617092j]]\n",
      "[[-0.47044318-0.17830651j -0.25417242+0.25096848j]\n",
      " [ 0.24565976+0.54553811j -0.44713503-0.39589326j]]\n",
      "[[-0.47027605-0.17881775j -0.25454447+0.25063737j]\n",
      " [ 0.24508318+0.54581381j -0.44663801-0.39644847j]]\n",
      "[[-0.47019983-0.17901955j -0.2544548 +0.25074617j]\n",
      " [ 0.24485246+0.54592624j -0.44675897-0.39629021j]]\n",
      "[[-0.47051909-0.17810464j -0.25426213+0.25085984j]\n",
      " [ 0.24589027+0.54542536j -0.44701431-0.39605154j]]\n",
      "[[-0.47019983-0.17901955j -0.2544548 +0.25074617j]\n",
      " [ 0.24485246+0.54592624j -0.44675897-0.39629021j]]\n",
      "[[-0.47051909-0.17810464j -0.25426213+0.25085984j]\n",
      " [ 0.24589027+0.54542536j -0.44701431-0.39605154j]]\n",
      "[[-0.47040562-0.17842352j -0.25404272+0.25110764j]\n",
      " [ 0.24553636+0.54559375j -0.44737582-0.39560834j]]\n",
      "[[-0.47031371-0.17870075j -0.25467388+0.25049798j]\n",
      " [ 0.24520665+0.54575826j -0.44639674-0.39673288j]]\n",
      "[[-0.47035969-0.17856214j -0.25435849+0.250803j  ]\n",
      " [ 0.24537151+0.54567603j -0.44688663-0.39617092j]]\n",
      "[[-0.47035969-0.17856214j -0.25435849+0.250803j  ]\n",
      " [ 0.24537151+0.54567603j -0.44688663-0.39617092j]]\n",
      "[[-0.47034724-0.17859279j -0.2544652 +0.250694j  ]\n",
      " [ 0.24532254+0.54568887j -0.44670257-0.39637939j]]\n",
      "[[-0.47037211-0.17853147j -0.25425228+0.25091139j]\n",
      " [ 0.24542046+0.54566319j -0.44706965-0.39596341j]]\n",
      "\n",
      "Numerical gradient of P with respect to Q:\n",
      "[[[[ 0.89228153 -0.9995699 ]\n",
      "   [-0.9995699   0.42021275]]\n",
      "\n",
      "  [[ 0.35360456  0.        ]\n",
      "   [ 0.          0.        ]]]\n",
      "\n",
      "\n",
      " [[[ 0.35360456  0.        ]\n",
      "   [ 0.          0.        ]]\n",
      "\n",
      "  [[ 0.21010637  0.        ]\n",
      "   [ 0.          0.29742718]]]]\n",
      "\n",
      "Numerical gradient of P with respect to R:\n",
      "[[[[0.06616116]]\n",
      "\n",
      "  [[0.06079674]]]\n",
      "\n",
      "\n",
      " [[[0.06079674]]\n",
      "\n",
      "  [[0.17434359]]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mk/miniforge3/envs/guam/lib/python3.10/site-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=complex64 to dtype=float32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/mk/miniforge3/envs/guam/lib/python3.10/site-packages/jax/_src/ops/scatter.py:134: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return lax_internal._convert_element_type(out, dtype, weak_type)\n"
     ]
    }
   ],
   "source": [
    "# Finite difference check for each entry of Q and R\n",
    "def numerical_jacobian(f, x, epsilon=1e-4):\n",
    "    \"\"\"Compute numerical Jacobian for each entry of x on matrix output f(x).\"\"\"\n",
    "    jacobian = jnp.zeros((f(x).shape[0], f(x).shape[1], x.shape[0], x.shape[1]))\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x_perturb_plus = x.at[i, j].set(x[i, j] + epsilon)\n",
    "            x_perturb_minus = x.at[i, j].set(x[i, j] - epsilon)\n",
    "            if i != j:\n",
    "                x_perturb_plus = x_perturb_plus.at[j,i].set(x_perturb_plus[j,i] + epsilon)\n",
    "                x_perturb_minus = x_perturb_minus.at[j,i].set(x_perturb_minus[j,i] - epsilon) \n",
    "            f_plus = f(x_perturb_plus)\n",
    "            f_minus = f(x_perturb_minus)\n",
    "            jacobian = jacobian.at[:, :, i, j].set((f_plus - f_minus) / (2 * epsilon))\n",
    "    return jacobian\n",
    "\n",
    "# Define function to get P with fixed A, B\n",
    "def get_P_with_Q(Q):\n",
    "    return lqr_solution(A, B, Q, R)\n",
    "\n",
    "def get_P_with_R(R):\n",
    "    return lqr_solution(A, B, Q, R)\n",
    "\n",
    "# Compute numerical gradients\n",
    "numerical_grad_Q = numerical_jacobian(get_P_with_Q, Q)\n",
    "numerical_grad_R = numerical_jacobian(get_P_with_R, R)\n",
    "\n",
    "print(\"\\nNumerical gradient of P with respect to Q:\")\n",
    "print(numerical_grad_Q)\n",
    "\n",
    "print(\"\\nNumerical gradient of P with respect to R:\")\n",
    "print(numerical_grad_R)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
